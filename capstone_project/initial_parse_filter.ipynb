{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('RS_filtered.json'):\n",
    "    print('RS_filtered.json not found, starting to parse RS_2016-11')\n",
    "    if os.path.exists('RS_2016-11'): \n",
    "        file = open(\"RS_2016-11\", \"r\")\n",
    "        filtered_subs = []\n",
    "        for idx, line in enumerate(file):\n",
    "            s = json.loads(line)\n",
    "            filtered_subs.append({\n",
    "                'num_comments': s['num_comments'],\n",
    "                'title': s['title'],\n",
    "                'created_utc': s['created_utc'],\n",
    "            })\n",
    "\n",
    "        json.dump(filtered_subs, open('RS_filtered.json', 'w'))\n",
    "    else:\n",
    "        print('RS_2016-11 not found, please download RS_2016-11.zst from')\n",
    "        print('http://files.pushshift.io/reddit/comments/')\n",
    "        print('and extract it using unzstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8660144\n",
      "dict_keys(['num_comments', 'title', 'created_utc'])\n"
     ]
    }
   ],
   "source": [
    "RS_data = json.load(open('RS_filtered.json', 'r'))\n",
    "print(len(RS_data))\n",
    "print(RS_data[0].keys())\n",
    "del RS_data  # Release the memory so the notebook kernel does not crash when reading the next file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('News_filtered.json'):\n",
    "    news_subdirs = ['news_714_20170904122143/714_webhose-2017-02_20170904122220',\n",
    "                    'news_714_20170904122143/714_webhose-2017-03_20170904122636',\n",
    "                    'eng_news624_20170904081113/624_webhose-2016-11_20170904081158'\n",
    "                   ]\n",
    "    filtered_news = []\n",
    "    for path in news_subdirs:\n",
    "        if os.path.exists(path):\n",
    "            onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "            for filepath in onlyfiles:\n",
    "                s = json.load(open(os.path.join(path, filepath), 'r'))\n",
    "                filtered_news.append({\n",
    "                    'likes': s['thread']['social']['facebook']['likes'],\n",
    "                    'text': s['text'],\n",
    "                    'published': s['published'],\n",
    "                })\n",
    "        else:\n",
    "            print(\"Path:\", path, \"not found\")\n",
    "            print(\"Please download from:\")\n",
    "            print(\"https://webhose.io/free-datasets/articles-by-virality/\")\n",
    "            print(\"https://webhose.io/free-datasets/language/\")\n",
    "\n",
    "    json.dump(filtered_news, open('News_filtered.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670492\n",
      "dict_keys(['likes', 'text', 'published'])\n"
     ]
    }
   ],
   "source": [
    "News_data = json.load(open('News_filtered.json', 'r'))\n",
    "print(len(News_data))\n",
    "print(News_data[0].keys())\n",
    "del News_data  # Release the memory so the notebook kernel does not crash when reading the next file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
